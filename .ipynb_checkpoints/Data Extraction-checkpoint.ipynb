{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43dd364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43a39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2707f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ffdf97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db033fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fe35a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Netclan20241017.txt\n",
      "Saved: Netclan20241018.txt\n",
      "Saved: Netclan20241019.txt\n",
      "Saved: Netclan20241020.txt\n",
      "Saved: Netclan20241021.txt\n",
      "Saved: Netclan20241022.txt\n",
      "Saved: Netclan20241023.txt\n",
      "Saved: Netclan20241024.txt\n",
      "Saved: Netclan20241025.txt\n",
      "Saved: Netclan20241026.txt\n",
      "Saved: Netclan20241027.txt\n",
      "Saved: Netclan20241028.txt\n",
      "Saved: Netclan20241029.txt\n",
      "Saved: Netclan20241030.txt\n",
      "Saved: Netclan20241031.txt\n",
      "Saved: Netclan20241032.txt\n",
      "Saved: Netclan20241033.txt\n",
      "Saved: Netclan20241034.txt\n",
      "Saved: Netclan20241035.txt\n",
      "Saved: Netclan20241036.txt\n",
      "Saved: Netclan20241037.txt\n",
      "Saved: Netclan20241038.txt\n",
      "Saved: Netclan20241039.txt\n",
      "Saved: Netclan20241040.txt\n",
      "Saved: Netclan20241041.txt\n",
      "Saved: Netclan20241042.txt\n",
      "Saved: Netclan20241043.txt\n",
      "Saved: Netclan20241044.txt\n",
      "Saved: Netclan20241045.txt\n",
      "Saved: Netclan20241046.txt\n",
      "Saved: Netclan20241047.txt\n",
      "Saved: Netclan20241048.txt\n",
      "Saved: Netclan20241049.txt\n",
      "Saved: Netclan20241050.txt\n",
      "Saved: Netclan20241051.txt\n",
      "Saved: Netclan20241052.txt\n",
      "Saved: Netclan20241053.txt\n",
      "Saved: Netclan20241054.txt\n",
      "Saved: Netclan20241055.txt\n",
      "Saved: Netclan20241056.txt\n",
      "Saved: Netclan20241057.txt\n",
      "Saved: Netclan20241058.txt\n",
      "Saved: Netclan20241059.txt\n",
      "Saved: Netclan20241060.txt\n",
      "Saved: Netclan20241061.txt\n",
      "Saved: Netclan20241062.txt\n",
      "Saved: Netclan20241063.txt\n",
      "Saved: Netclan20241064.txt\n",
      "Saved: Netclan20241065.txt\n",
      "Saved: Netclan20241066.txt\n",
      "Saved: Netclan20241067.txt\n",
      "Saved: Netclan20241068.txt\n",
      "Saved: Netclan20241069.txt\n",
      "Saved: Netclan20241070.txt\n",
      "Saved: Netclan20241071.txt\n",
      "Saved: Netclan20241072.txt\n",
      "Saved: Netclan20241073.txt\n",
      "Saved: Netclan20241074.txt\n",
      "Saved: Netclan20241075.txt\n",
      "Saved: Netclan20241076.txt\n",
      "Saved: Netclan20241077.txt\n",
      "Saved: Netclan20241078.txt\n",
      "Saved: Netclan20241079.txt\n",
      "Saved: Netclan20241080.txt\n",
      "Saved: Netclan20241081.txt\n",
      "Saved: Netclan20241082.txt\n",
      "Saved: Netclan20241083.txt\n",
      "Saved: Netclan20241084.txt\n",
      "Saved: Netclan20241085.txt\n",
      "Saved: Netclan20241086.txt\n",
      "Saved: Netclan20241087.txt\n",
      "Saved: Netclan20241088.txt\n",
      "Saved: Netclan20241089.txt\n",
      "Saved: Netclan20241090.txt\n",
      "Saved: Netclan20241091.txt\n",
      "Saved: Netclan20241092.txt\n",
      "Saved: Netclan20241093.txt\n",
      "Saved: Netclan20241094.txt\n",
      "Saved: Netclan20241095.txt\n",
      "Saved: Netclan20241096.txt\n",
      "Saved: Netclan20241097.txt\n",
      "Saved: Netclan20241098.txt\n",
      "Saved: Netclan20241099.txt\n",
      "Saved: Netclan20241100.txt\n",
      "Saved: Netclan20241101.txt\n",
      "Saved: Netclan20241102.txt\n",
      "Saved: Netclan20241103.txt\n",
      "Saved: Netclan20241104.txt\n",
      "Saved: Netclan20241105.txt\n",
      "Saved: Netclan20241106.txt\n",
      "Saved: Netclan20241107.txt\n",
      "Saved: Netclan20241108.txt\n",
      "Saved: Netclan20241109.txt\n",
      "Saved: Netclan20241110.txt\n",
      "Saved: Netclan20241111.txt\n",
      "Saved: Netclan20241112.txt\n",
      "Saved: Netclan20241113.txt\n",
      "Saved: Netclan20241114.txt\n",
      "Saved: Netclan20241115.txt\n",
      "Saved: Netclan20241116.txt\n",
      "Saved: Netclan20241117.txt\n",
      "Saved: Netclan20241118.txt\n",
      "Saved: Netclan20241119.txt\n",
      "Saved: Netclan20241120.txt\n",
      "Saved: Netclan20241121.txt\n",
      "Saved: Netclan20241122.txt\n",
      "Saved: Netclan20241123.txt\n",
      "Saved: Netclan20241124.txt\n",
      "Saved: Netclan20241125.txt\n",
      "Saved: Netclan20241126.txt\n",
      "Saved: Netclan20241127.txt\n",
      "Saved: Netclan20241128.txt\n",
      "Saved: Netclan20241129.txt\n",
      "Saved: Netclan20241130.txt\n",
      "Saved: Netclan20241131.txt\n",
      "Saved: Netclan20241132.txt\n",
      "Saved: Netclan20241133.txt\n",
      "Saved: Netclan20241134.txt\n",
      "Saved: Netclan20241135.txt\n",
      "Saved: Netclan20241136.txt\n",
      "Saved: Netclan20241137.txt\n",
      "Saved: Netclan20241138.txt\n",
      "Saved: Netclan20241139.txt\n",
      "Saved: Netclan20241140.txt\n",
      "Saved: Netclan20241141.txt\n",
      "Saved: Netclan20241142.txt\n",
      "Saved: Netclan20241143.txt\n",
      "Saved: Netclan20241144.txt\n",
      "Saved: Netclan20241145.txt\n",
      "Saved: Netclan20241146.txt\n",
      "Saved: Netclan20241147.txt\n",
      "Saved: Netclan20241148.txt\n",
      "Saved: Netclan20241149.txt\n",
      "Saved: Netclan20241150.txt\n",
      "Saved: Netclan20241151.txt\n",
      "Saved: Netclan20241152.txt\n",
      "Saved: Netclan20241153.txt\n",
      "Saved: Netclan20241154.txt\n",
      "Saved: Netclan20241155.txt\n",
      "Saved: Netclan20241156.txt\n",
      "Saved: Netclan20241157.txt\n",
      "Saved: Netclan20241158.txt\n",
      "Saved: Netclan20241159.txt\n",
      "Saved: Netclan20241160.txt\n",
      "Saved: Netclan20241161.txt\n",
      "Saved: Netclan20241162.txt\n",
      "Saved: Netclan20241163.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Create folders\n",
    "os.makedirs(\"data/articles\", exist_ok=True)\n",
    "\n",
    "# Load input file\n",
    "input_file = r\"C:\\Users\\jasmi\\Jasmine\\Data extraction\\Input.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "urls = df[['URL_ID', 'URL']]\n",
    "\n",
    "# Function to extract content\n",
    "def extract_text(url):\n",
    "    \"\"\"Extracts the title and main content from the webpage.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract title and content\n",
    "        title = soup.title.text.strip() if soup.title else \"No Title\"\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "\n",
    "        return title, article_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Loop through each URL\n",
    "for index, row in urls.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    title, content = extract_text(url)\n",
    "\n",
    "    if content:\n",
    "        with open(f\"data/articles/{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"{title}\\n\\n{content}\")\n",
    "        print(f\"Saved: {url_id}.txt\")\n",
    "    else:\n",
    "        print(f\"Failed to save: {url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a3e392b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jasmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jasmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import textstat\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import textstat\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load positive and negative words\n",
    "with open(r\"C:\\Users\\jasmi\\Jasmine\\Data extraction\\positive-words.txt\", \"r\") as f:\n",
    "    positive_words = set(f.read().split())\n",
    "\n",
    "with open(r\"C:\\Users\\jasmi\\Jasmine\\Data extraction\\positive-words.txt\", \"r\") as f:\n",
    "    negative_words = set(f.read().split())\n",
    "\n",
    "def analyze_text(file_path):\n",
    "    \"\"\"Computes the required variables for the given article.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    total_words = len(words)\n",
    "\n",
    "    # POSITIVE & NEGATIVE SCORE\n",
    "    positive_score = sum(1 for word in words if word.lower() in positive_words)\n",
    "    negative_score = sum(1 for word in words if word.lower() in negative_words)\n",
    "\n",
    "    # POLARITY AND SUBJECTIVITY\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.0001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words + 0.0001)\n",
    "\n",
    "    # SENTENCE LENGTH & COMPLEXITY\n",
    "    avg_sentence_length = total_words / len(sentences)\n",
    "    complex_words = [word for word in words if textstat.syllable_count(word) > 2]\n",
    "    percentage_complex_words = len(complex_words) / total_words * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # WORD & SYLLABLE METRICS\n",
    "    syllable_per_word = sum(textstat.syllable_count(word) for word in words) / total_words\n",
    "    avg_word_length = sum(len(word) for word in words) / total_words\n",
    "\n",
    "    # PERSONAL PRONOUNS\n",
    "    pronouns = re.findall(r'\\b(I|we|my|ours|us|me|mine|your|you)\\b', text, re.I)\n",
    "    personal_pronouns = len(pronouns)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"positive_score\": positive_score,\n",
    "        \"negative_score\": negative_score,\n",
    "        \"polarity_score\": polarity_score,\n",
    "        \"subjectivity_score\": subjectivity_score,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"percentage_complex_words\": percentage_complex_words,\n",
    "        \"fog_index\": fog_index,\n",
    "        \"complex_word_count\": len(complex_words),\n",
    "        \"word_count\": total_words,\n",
    "        \"syllable_per_word\": syllable_per_word,\n",
    "        \"personal_pronouns\": personal_pronouns,\n",
    "        \"avg_word_length\": avg_word_length\n",
    "    }\n",
    "\n",
    "# Load positive and negative words\n",
    "with open(r\"C:\\Users\\jasmi\\Jasmine\\Data extraction\\positive-words.txt\", \"r\") as f:\n",
    "    positive_words = set(f.read().split())\n",
    "\n",
    "with open(r\"C:\\Users\\jasmi\\Jasmine\\Data extraction\\positive-words.txt\", \"r\") as f:\n",
    "    negative_words = set(f.read().split())\n",
    "\n",
    "def analyze_text(file_path):\n",
    "    \"\"\"Computes the required variables for the given article.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    total_words = len(words)\n",
    "\n",
    "    # POSITIVE & NEGATIVE SCORE\n",
    "    positive_score = sum(1 for word in words if word.lower() in positive_words)\n",
    "    negative_score = sum(1 for word in words if word.lower() in negative_words)\n",
    "\n",
    "    # POLARITY AND SUBJECTIVITY\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.0001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words + 0.0001)\n",
    "\n",
    "    # SENTENCE LENGTH & COMPLEXITY\n",
    "    avg_sentence_length = total_words / len(sentences)\n",
    "    complex_words = [word for word in words if textstat.syllable_count(word) > 2]\n",
    "    percentage_complex_words = len(complex_words) / total_words * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # WORD & SYLLABLE METRICS\n",
    "    syllable_per_word = sum(textstat.syllable_count(word) for word in words) / total_words\n",
    "    avg_word_length = sum(len(word) for word in words) / total_words\n",
    "\n",
    "    # PERSONAL PRONOUNS\n",
    "    pronouns = re.findall(r'\\b(I|we|my|ours|us|me|mine|your|you)\\b', text, re.I)\n",
    "    personal_pronouns = len(pronouns)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"positive_score\": positive_score,\n",
    "        \"negative_score\": negative_score,\n",
    "        \"polarity_score\": polarity_score,\n",
    "        \"subjectivity_score\": subjectivity_score,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"percentage_complex_words\": percentage_complex_words,\n",
    "        \"fog_index\": fog_index,\n",
    "        \"complex_word_count\": len(complex_words),\n",
    "        \"word_count\": total_words,\n",
    "        \"syllable_per_word\": syllable_per_word,\n",
    "        \"personal_pronouns\": personal_pronouns,\n",
    "        \"avg_word_length\": avg_word_length\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f0525fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved successfully!\n"
     ]
    }
   ],
   "source": [
    "output_data = []\n",
    "\n",
    "for index, row in urls.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    file_path = f\"data/articles/{url_id}.txt\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        metrics = analyze_text(file_path)\n",
    "        row_data = {**row.to_dict(), **metrics}\n",
    "        output_data.append(row_data)\n",
    "\n",
    "# Save to Excel\n",
    "output_df = pd.DataFrame(output_data)\n",
    "output_df.to_excel(r\"C:\\Users\\jasmi\\Jasmine\\Data extraction\\Output Data Structure.xlsx\", index=False)\n",
    "print(\"Output saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
